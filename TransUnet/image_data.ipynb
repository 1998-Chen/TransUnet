{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "320"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(os.listdir(r'Image_2/train/train-org-img/'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1445"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(os.listdir(r'Image/train/train-label-img/'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_image = []\n",
    "for name in os.listdir(r'Image/train/train-org-img/'):\n",
    "    train_image.append(name.split('.')[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_label = []\n",
    "for name in os.listdir(r'Image/train/train-label-img/'):\n",
    "    train_label.append(name.split('_')[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "222"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 交集\n",
    "train_ = list(set(train_image) & set(train_label))\n",
    "len(train_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[36, 52, 41],\n",
       "        [35, 51, 40],\n",
       "        [19, 36, 25],\n",
       "        ...,\n",
       "        [32, 48, 37],\n",
       "        [30, 44, 32],\n",
       "        [28, 42, 30]],\n",
       "\n",
       "       [[35, 51, 40],\n",
       "        [38, 54, 43],\n",
       "        [23, 40, 29],\n",
       "        ...,\n",
       "        [31, 47, 36],\n",
       "        [32, 46, 34],\n",
       "        [29, 43, 31]],\n",
       "\n",
       "       [[37, 53, 42],\n",
       "        [43, 59, 48],\n",
       "        [35, 52, 41],\n",
       "        ...,\n",
       "        [34, 50, 39],\n",
       "        [31, 48, 35],\n",
       "        [27, 44, 31]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[26, 54, 41],\n",
       "        [23, 51, 38],\n",
       "        [28, 56, 43],\n",
       "        ...,\n",
       "        [38, 86, 80],\n",
       "        [35, 83, 77],\n",
       "        [32, 80, 74]],\n",
       "\n",
       "       [[26, 54, 41],\n",
       "        [23, 51, 38],\n",
       "        [26, 54, 41],\n",
       "        ...,\n",
       "        [37, 85, 79],\n",
       "        [34, 82, 76],\n",
       "        [33, 81, 75]],\n",
       "\n",
       "       [[26, 54, 41],\n",
       "        [22, 50, 37],\n",
       "        [23, 51, 38],\n",
       "        ...,\n",
       "        [32, 80, 74],\n",
       "        [31, 79, 73],\n",
       "        [34, 82, 76]]], dtype=uint8)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv2.imread(r'Image/train/train-org-img/'+train_[0]+'.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8305\n",
      "torch.Size([16, 3, 128, 256]) torch.Size([16, 1, 128, 256])\n",
      "torch.Size([16, 3, 128, 256]) torch.Size([16, 1, 128, 256])\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "\u001B[1;32me:\\Code_Project\\Perceiver\\TransUnet\\image_data.ipynb Cell 8\u001B[0m in \u001B[0;36m<cell line: 45>\u001B[1;34m()\u001B[0m\n\u001B[0;32m     <a href='vscode-notebook-cell:/e%3A/Code_Project/Perceiver/TransUnet/image_data.ipynb#X15sZmlsZQ%3D%3D?line=48'>49</a>\u001B[0m train_set \u001B[39m=\u001B[39m image_dataset(image_path\u001B[39m=\u001B[39m\u001B[39mr\u001B[39m\u001B[39m'\u001B[39m\u001B[39mImage/train/train-org-img/\u001B[39m\u001B[39m'\u001B[39m, label_path\u001B[39m=\u001B[39m\u001B[39mr\u001B[39m\u001B[39m'\u001B[39m\u001B[39mImage/train/train-label-img/\u001B[39m\u001B[39m'\u001B[39m, x\u001B[39m=\u001B[39mdf_list,\n\u001B[0;32m     <a href='vscode-notebook-cell:/e%3A/Code_Project/Perceiver/TransUnet/image_data.ipynb#X15sZmlsZQ%3D%3D?line=49'>50</a>\u001B[0m                           mean\u001B[39m=\u001B[39m[\u001B[39m0.485\u001B[39m, \u001B[39m0.456\u001B[39m, \u001B[39m0.406\u001B[39m], std\u001B[39m=\u001B[39m[\u001B[39m0.229\u001B[39m, \u001B[39m0.224\u001B[39m, \u001B[39m0.225\u001B[39m])\n\u001B[0;32m     <a href='vscode-notebook-cell:/e%3A/Code_Project/Perceiver/TransUnet/image_data.ipynb#X15sZmlsZQ%3D%3D?line=50'>51</a>\u001B[0m train_loader \u001B[39m=\u001B[39m DataLoader(train_set, batch_size\u001B[39m=\u001B[39m\u001B[39m16\u001B[39m, shuffle\u001B[39m=\u001B[39m\u001B[39mFalse\u001B[39;00m)\n\u001B[1;32m---> <a href='vscode-notebook-cell:/e%3A/Code_Project/Perceiver/TransUnet/image_data.ipynb#X15sZmlsZQ%3D%3D?line=52'>53</a>\u001B[0m \u001B[39mfor\u001B[39;00m i, (images, labels) \u001B[39min\u001B[39;00m \u001B[39menumerate\u001B[39m(train_loader):\n\u001B[0;32m     <a href='vscode-notebook-cell:/e%3A/Code_Project/Perceiver/TransUnet/image_data.ipynb#X15sZmlsZQ%3D%3D?line=53'>54</a>\u001B[0m     \u001B[39mprint\u001B[39m(images\u001B[39m.\u001B[39mshape, labels\u001B[39m.\u001B[39mshape)\n",
      "File \u001B[1;32mf:\\python\\miniconda3\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:681\u001B[0m, in \u001B[0;36m_BaseDataLoaderIter.__next__\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    678\u001B[0m \u001B[39mif\u001B[39;00m \u001B[39mself\u001B[39m\u001B[39m.\u001B[39m_sampler_iter \u001B[39mis\u001B[39;00m \u001B[39mNone\u001B[39;00m:\n\u001B[0;32m    679\u001B[0m     \u001B[39m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001B[39;00m\n\u001B[0;32m    680\u001B[0m     \u001B[39mself\u001B[39m\u001B[39m.\u001B[39m_reset()  \u001B[39m# type: ignore[call-arg]\u001B[39;00m\n\u001B[1;32m--> 681\u001B[0m data \u001B[39m=\u001B[39m \u001B[39mself\u001B[39;49m\u001B[39m.\u001B[39;49m_next_data()\n\u001B[0;32m    682\u001B[0m \u001B[39mself\u001B[39m\u001B[39m.\u001B[39m_num_yielded \u001B[39m+\u001B[39m\u001B[39m=\u001B[39m \u001B[39m1\u001B[39m\n\u001B[0;32m    683\u001B[0m \u001B[39mif\u001B[39;00m \u001B[39mself\u001B[39m\u001B[39m.\u001B[39m_dataset_kind \u001B[39m==\u001B[39m _DatasetKind\u001B[39m.\u001B[39mIterable \u001B[39mand\u001B[39;00m \\\n\u001B[0;32m    684\u001B[0m         \u001B[39mself\u001B[39m\u001B[39m.\u001B[39m_IterableDataset_len_called \u001B[39mis\u001B[39;00m \u001B[39mnot\u001B[39;00m \u001B[39mNone\u001B[39;00m \u001B[39mand\u001B[39;00m \\\n\u001B[0;32m    685\u001B[0m         \u001B[39mself\u001B[39m\u001B[39m.\u001B[39m_num_yielded \u001B[39m>\u001B[39m \u001B[39mself\u001B[39m\u001B[39m.\u001B[39m_IterableDataset_len_called:\n",
      "File \u001B[1;32mf:\\python\\miniconda3\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:721\u001B[0m, in \u001B[0;36m_SingleProcessDataLoaderIter._next_data\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    719\u001B[0m \u001B[39mdef\u001B[39;00m \u001B[39m_next_data\u001B[39m(\u001B[39mself\u001B[39m):\n\u001B[0;32m    720\u001B[0m     index \u001B[39m=\u001B[39m \u001B[39mself\u001B[39m\u001B[39m.\u001B[39m_next_index()  \u001B[39m# may raise StopIteration\u001B[39;00m\n\u001B[1;32m--> 721\u001B[0m     data \u001B[39m=\u001B[39m \u001B[39mself\u001B[39;49m\u001B[39m.\u001B[39;49m_dataset_fetcher\u001B[39m.\u001B[39;49mfetch(index)  \u001B[39m# may raise StopIteration\u001B[39;00m\n\u001B[0;32m    722\u001B[0m     \u001B[39mif\u001B[39;00m \u001B[39mself\u001B[39m\u001B[39m.\u001B[39m_pin_memory:\n\u001B[0;32m    723\u001B[0m         data \u001B[39m=\u001B[39m _utils\u001B[39m.\u001B[39mpin_memory\u001B[39m.\u001B[39mpin_memory(data, \u001B[39mself\u001B[39m\u001B[39m.\u001B[39m_pin_memory_device)\n",
      "File \u001B[1;32mf:\\python\\miniconda3\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:49\u001B[0m, in \u001B[0;36m_MapDatasetFetcher.fetch\u001B[1;34m(self, possibly_batched_index)\u001B[0m\n\u001B[0;32m     47\u001B[0m \u001B[39mdef\u001B[39;00m \u001B[39mfetch\u001B[39m(\u001B[39mself\u001B[39m, possibly_batched_index):\n\u001B[0;32m     48\u001B[0m     \u001B[39mif\u001B[39;00m \u001B[39mself\u001B[39m\u001B[39m.\u001B[39mauto_collation:\n\u001B[1;32m---> 49\u001B[0m         data \u001B[39m=\u001B[39m [\u001B[39mself\u001B[39m\u001B[39m.\u001B[39mdataset[idx] \u001B[39mfor\u001B[39;00m idx \u001B[39min\u001B[39;00m possibly_batched_index]\n\u001B[0;32m     50\u001B[0m     \u001B[39melse\u001B[39;00m:\n\u001B[0;32m     51\u001B[0m         data \u001B[39m=\u001B[39m \u001B[39mself\u001B[39m\u001B[39m.\u001B[39mdataset[possibly_batched_index]\n",
      "File \u001B[1;32mf:\\python\\miniconda3\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:49\u001B[0m, in \u001B[0;36m<listcomp>\u001B[1;34m(.0)\u001B[0m\n\u001B[0;32m     47\u001B[0m \u001B[39mdef\u001B[39;00m \u001B[39mfetch\u001B[39m(\u001B[39mself\u001B[39m, possibly_batched_index):\n\u001B[0;32m     48\u001B[0m     \u001B[39mif\u001B[39;00m \u001B[39mself\u001B[39m\u001B[39m.\u001B[39mauto_collation:\n\u001B[1;32m---> 49\u001B[0m         data \u001B[39m=\u001B[39m [\u001B[39mself\u001B[39;49m\u001B[39m.\u001B[39;49mdataset[idx] \u001B[39mfor\u001B[39;00m idx \u001B[39min\u001B[39;00m possibly_batched_index]\n\u001B[0;32m     50\u001B[0m     \u001B[39melse\u001B[39;00m:\n\u001B[0;32m     51\u001B[0m         data \u001B[39m=\u001B[39m \u001B[39mself\u001B[39m\u001B[39m.\u001B[39mdataset[possibly_batched_index]\n",
      "\u001B[1;32me:\\Code_Project\\Perceiver\\TransUnet\\image_data.ipynb Cell 8\u001B[0m in \u001B[0;36mimage_dataset.__getitem__\u001B[1;34m(self, idx)\u001B[0m\n\u001B[0;32m     <a href='vscode-notebook-cell:/e%3A/Code_Project/Perceiver/TransUnet/image_data.ipynb#X15sZmlsZQ%3D%3D?line=37'>38</a>\u001B[0m img \u001B[39m=\u001B[39m cv2\u001B[39m.\u001B[39mcvtColor(img, cv2\u001B[39m.\u001B[39mCOLOR_BGR2RGB)\n\u001B[0;32m     <a href='vscode-notebook-cell:/e%3A/Code_Project/Perceiver/TransUnet/image_data.ipynb#X15sZmlsZQ%3D%3D?line=38'>39</a>\u001B[0m label \u001B[39m=\u001B[39m cv2\u001B[39m.\u001B[39mimread(\u001B[39mself\u001B[39m\u001B[39m.\u001B[39mlabel_path \u001B[39m+\u001B[39m \u001B[39mself\u001B[39m\u001B[39m.\u001B[39mx[idx] \u001B[39m+\u001B[39m \u001B[39m'\u001B[39m\u001B[39m_lab.png\u001B[39m\u001B[39m'\u001B[39m, cv2\u001B[39m.\u001B[39mIMREAD_GRAYSCALE)  \u001B[39m# cv2.IMREAD_GRAYSCALE np.ndaary()\u001B[39;00m\n\u001B[1;32m---> <a href='vscode-notebook-cell:/e%3A/Code_Project/Perceiver/TransUnet/image_data.ipynb#X15sZmlsZQ%3D%3D?line=39'>40</a>\u001B[0m img \u001B[39m=\u001B[39m \u001B[39mself\u001B[39;49m\u001B[39m.\u001B[39;49mtransform_image(img)\n\u001B[0;32m     <a href='vscode-notebook-cell:/e%3A/Code_Project/Perceiver/TransUnet/image_data.ipynb#X15sZmlsZQ%3D%3D?line=40'>41</a>\u001B[0m label \u001B[39m=\u001B[39m \u001B[39mself\u001B[39m\u001B[39m.\u001B[39mtransform_label(label)\n\u001B[0;32m     <a href='vscode-notebook-cell:/e%3A/Code_Project/Perceiver/TransUnet/image_data.ipynb#X15sZmlsZQ%3D%3D?line=41'>42</a>\u001B[0m \u001B[39mreturn\u001B[39;00m img, label\n",
      "File \u001B[1;32mf:\\python\\miniconda3\\lib\\site-packages\\torchvision\\transforms\\transforms.py:94\u001B[0m, in \u001B[0;36mCompose.__call__\u001B[1;34m(self, img)\u001B[0m\n\u001B[0;32m     92\u001B[0m \u001B[39mdef\u001B[39;00m \u001B[39m__call__\u001B[39m(\u001B[39mself\u001B[39m, img):\n\u001B[0;32m     93\u001B[0m     \u001B[39mfor\u001B[39;00m t \u001B[39min\u001B[39;00m \u001B[39mself\u001B[39m\u001B[39m.\u001B[39mtransforms:\n\u001B[1;32m---> 94\u001B[0m         img \u001B[39m=\u001B[39m t(img)\n\u001B[0;32m     95\u001B[0m     \u001B[39mreturn\u001B[39;00m img\n",
      "File \u001B[1;32mf:\\python\\miniconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *input, **kwargs)\u001B[0m\n\u001B[0;32m   1126\u001B[0m \u001B[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1127\u001B[0m \u001B[39m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1128\u001B[0m \u001B[39mif\u001B[39;00m \u001B[39mnot\u001B[39;00m (\u001B[39mself\u001B[39m\u001B[39m.\u001B[39m_backward_hooks \u001B[39mor\u001B[39;00m \u001B[39mself\u001B[39m\u001B[39m.\u001B[39m_forward_hooks \u001B[39mor\u001B[39;00m \u001B[39mself\u001B[39m\u001B[39m.\u001B[39m_forward_pre_hooks \u001B[39mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1129\u001B[0m         \u001B[39mor\u001B[39;00m _global_forward_hooks \u001B[39mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1130\u001B[0m     \u001B[39mreturn\u001B[39;00m forward_call(\u001B[39m*\u001B[39m\u001B[39minput\u001B[39m, \u001B[39m*\u001B[39m\u001B[39m*\u001B[39mkwargs)\n\u001B[0;32m   1131\u001B[0m \u001B[39m# Do not call functions when jit is used\u001B[39;00m\n\u001B[0;32m   1132\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[39m=\u001B[39m [], []\n",
      "File \u001B[1;32mf:\\python\\miniconda3\\lib\\site-packages\\torchvision\\transforms\\transforms.py:349\u001B[0m, in \u001B[0;36mResize.forward\u001B[1;34m(self, img)\u001B[0m\n\u001B[0;32m    341\u001B[0m \u001B[39mdef\u001B[39;00m \u001B[39mforward\u001B[39m(\u001B[39mself\u001B[39m, img):\n\u001B[0;32m    342\u001B[0m     \u001B[39m\"\"\"\u001B[39;00m\n\u001B[0;32m    343\u001B[0m \u001B[39m    Args:\u001B[39;00m\n\u001B[0;32m    344\u001B[0m \u001B[39m        img (PIL Image or Tensor): Image to be scaled.\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    347\u001B[0m \u001B[39m        PIL Image or Tensor: Rescaled image.\u001B[39;00m\n\u001B[0;32m    348\u001B[0m \u001B[39m    \"\"\"\u001B[39;00m\n\u001B[1;32m--> 349\u001B[0m     \u001B[39mreturn\u001B[39;00m F\u001B[39m.\u001B[39;49mresize(img, \u001B[39mself\u001B[39;49m\u001B[39m.\u001B[39;49msize, \u001B[39mself\u001B[39;49m\u001B[39m.\u001B[39;49minterpolation, \u001B[39mself\u001B[39;49m\u001B[39m.\u001B[39;49mmax_size, \u001B[39mself\u001B[39;49m\u001B[39m.\u001B[39;49mantialias)\n",
      "File \u001B[1;32mf:\\python\\miniconda3\\lib\\site-packages\\torchvision\\transforms\\functional.py:432\u001B[0m, in \u001B[0;36mresize\u001B[1;34m(img, size, interpolation, max_size, antialias)\u001B[0m\n\u001B[0;32m    429\u001B[0m     pil_interpolation \u001B[39m=\u001B[39m pil_modes_mapping[interpolation]\n\u001B[0;32m    430\u001B[0m     \u001B[39mreturn\u001B[39;00m F_pil\u001B[39m.\u001B[39mresize(img, size\u001B[39m=\u001B[39msize, interpolation\u001B[39m=\u001B[39mpil_interpolation, max_size\u001B[39m=\u001B[39mmax_size)\n\u001B[1;32m--> 432\u001B[0m \u001B[39mreturn\u001B[39;00m F_t\u001B[39m.\u001B[39;49mresize(img, size\u001B[39m=\u001B[39;49msize, interpolation\u001B[39m=\u001B[39;49minterpolation\u001B[39m.\u001B[39;49mvalue, max_size\u001B[39m=\u001B[39;49mmax_size, antialias\u001B[39m=\u001B[39;49mantialias)\n",
      "File \u001B[1;32mf:\\python\\miniconda3\\lib\\site-packages\\torchvision\\transforms\\functional_tensor.py:491\u001B[0m, in \u001B[0;36mresize\u001B[1;34m(img, size, interpolation, max_size, antialias)\u001B[0m\n\u001B[0;32m    488\u001B[0m \u001B[39melse\u001B[39;00m:  \u001B[39m# specified both h and w\u001B[39;00m\n\u001B[0;32m    489\u001B[0m     new_w, new_h \u001B[39m=\u001B[39m size[\u001B[39m1\u001B[39m], size[\u001B[39m0\u001B[39m]\n\u001B[1;32m--> 491\u001B[0m img, need_cast, need_squeeze, out_dtype \u001B[39m=\u001B[39m _cast_squeeze_in(img, [torch\u001B[39m.\u001B[39;49mfloat32, torch\u001B[39m.\u001B[39;49mfloat64])\n\u001B[0;32m    493\u001B[0m \u001B[39m# Define align_corners to avoid warnings\u001B[39;00m\n\u001B[0;32m    494\u001B[0m align_corners \u001B[39m=\u001B[39m \u001B[39mFalse\u001B[39;00m \u001B[39mif\u001B[39;00m interpolation \u001B[39min\u001B[39;00m [\u001B[39m\"\u001B[39m\u001B[39mbilinear\u001B[39m\u001B[39m\"\u001B[39m, \u001B[39m\"\u001B[39m\u001B[39mbicubic\u001B[39m\u001B[39m\"\u001B[39m] \u001B[39melse\u001B[39;00m \u001B[39mNone\u001B[39;00m\n",
      "File \u001B[1;32mf:\\python\\miniconda3\\lib\\site-packages\\torchvision\\transforms\\functional_tensor.py:549\u001B[0m, in \u001B[0;36m_cast_squeeze_in\u001B[1;34m(img, req_dtypes)\u001B[0m\n\u001B[0;32m    547\u001B[0m \u001B[39m# make image NCHW\u001B[39;00m\n\u001B[0;32m    548\u001B[0m \u001B[39mif\u001B[39;00m img\u001B[39m.\u001B[39mndim \u001B[39m<\u001B[39m \u001B[39m4\u001B[39m:\n\u001B[1;32m--> 549\u001B[0m     img \u001B[39m=\u001B[39m img\u001B[39m.\u001B[39;49munsqueeze(dim\u001B[39m=\u001B[39;49m\u001B[39m0\u001B[39;49m)\n\u001B[0;32m    550\u001B[0m     need_squeeze \u001B[39m=\u001B[39m \u001B[39mTrue\u001B[39;00m\n\u001B[0;32m    552\u001B[0m out_dtype \u001B[39m=\u001B[39m img\u001B[39m.\u001B[39mdtype\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "import os \n",
    "import cv2\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision.transforms import Compose, ToTensor, Normalize, Resize\n",
    "\n",
    "def create_df(image_path, label_path):\n",
    "\n",
    "    train_image, train_label = [], []\n",
    "    for name in os.listdir(image_path):\n",
    "        train_image.append(name.split('.')[0])\n",
    "\n",
    "    for name in os.listdir(label_path):\n",
    "        train_label.append(name.split('_')[0])\n",
    "\n",
    "    # 交集\n",
    "    train_df = list(set(train_image) & set(train_label))\n",
    "    return train_df\n",
    "\n",
    "\n",
    "class image_dataset(Dataset):\n",
    "    def __init__(self, image_path, label_path, x, mean, std) -> None:\n",
    "        super().__init__()\n",
    "        self.img_path = image_path\n",
    "        self.label_path = label_path\n",
    "        self.x = x\n",
    "        self.mean = mean\n",
    "        self.std = std\n",
    "        self.transform_image = Compose([ToTensor(), Normalize(self.mean, self.std), Resize((128, 256))])\n",
    "        self.transform_label = Compose([ToTensor(), Normalize(0, 1/255), Resize((128, 256))])\n",
    "\n",
    "    def __len__(self, ):\n",
    "        return len(self.x)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img = cv2.imread(self.img_path + self.x[idx] + '.jpg')\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        label = cv2.imread(self.label_path + self.x[idx] + '_lab.png', cv2.IMREAD_GRAYSCALE)  # cv2.IMREAD_GRAYSCALE np.ndaary()\n",
    "        img = self.transform_image(img)\n",
    "        label = self.transform_label(label).squeeze(0)\n",
    "        return img, label\n",
    "    \n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    df_list = create_df(image_path='Image/train/train-org-img/', label_path='Image/train/train-label-img/')\n",
    "    print(df_list[0])\n",
    "    train_set = image_dataset(image_path=r'Image/train/train-org-img/', label_path=r'Image/train/train-label-img/', x=df_list,\n",
    "                              mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    train_loader = DataLoader(train_set, batch_size=16, shuffle=False)\n",
    "\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        print(images.shape, labels.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}